{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll explore how to use scikit-learn's `GridSearchCV` class to exhaustively search through every combination hyperparameters until we find the values for a given model.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Understand and explain parameter tuning and why it is necessary \n",
    "* Design and create a parameter grid for use with sklearn's GridSearchCV module\n",
    "* Use GridSearchCV to increase model performance through parameter tuning\n",
    "\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "For this lab, we'll be working with the [Wine Quality Dataset](https://archive.ics.uci.edu/ml/datasets/wine+quality) from the UCI Machine Learning Dataset Repository.  We'll be using data about the various features of wine to predict the quality of the wine on a scale from 1-10 stars, making this a multiclass classification problem.  \n",
    "\n",
    "### Getting Started\n",
    "\n",
    "Before we can begin GridSearching our way to optimal hyperparameters, we'll need to go through the basic steps of modeling.  This means that we'll need to:\n",
    "\n",
    "* Import and inspect the dataset (and clean, if necessary)\n",
    "* Split the data into training and testing sets\n",
    "* Build and fit a baseline model that we can compare against our GridSearch results.\n",
    "\n",
    "Run the cell below to import everything we'll need for this lab.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've imported all the necessary libraries and frameworks for this lab, we'll need to get the dataset.  \n",
    "\n",
    "Our data is stored in the file `winequality-red.csv`. Use pandas to import the data from this file and store it in a DataFrame.  Print the head to ensure that everything loaded correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.069</td>\n",
       "      <td>15.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.99640</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.065</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.99460</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.073</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.57</td>\n",
       "      <td>9.5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.071</td>\n",
       "      <td>17.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.80</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.097</td>\n",
       "      <td>15.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.99590</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.071</td>\n",
       "      <td>17.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.80</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.6</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.089</td>\n",
       "      <td>16.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.99430</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>9.9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.114</td>\n",
       "      <td>9.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.99740</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.56</td>\n",
       "      <td>9.1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.9</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.18</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.176</td>\n",
       "      <td>52.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.99860</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.88</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8.9</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.19</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.170</td>\n",
       "      <td>51.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.99860</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.93</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.5</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.092</td>\n",
       "      <td>35.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.99690</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.75</td>\n",
       "      <td>10.5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.368</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.11</td>\n",
       "      <td>1.28</td>\n",
       "      <td>9.3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.086</td>\n",
       "      <td>6.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.99740</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.341</td>\n",
       "      <td>17.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.99690</td>\n",
       "      <td>3.04</td>\n",
       "      <td>1.08</td>\n",
       "      <td>9.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.9</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.077</td>\n",
       "      <td>29.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.082</td>\n",
       "      <td>23.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.99820</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.106</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.99660</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.91</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.5</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.084</td>\n",
       "      <td>9.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.085</td>\n",
       "      <td>21.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.63</td>\n",
       "      <td>9.7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.080</td>\n",
       "      <td>11.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.99550</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.080</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.99620</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.106</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.99660</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.91</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.080</td>\n",
       "      <td>14.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.99720</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.55</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.082</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.99640</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.056</td>\n",
       "      <td>15.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99396</td>\n",
       "      <td>3.48</td>\n",
       "      <td>0.57</td>\n",
       "      <td>11.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.230</td>\n",
       "      <td>19.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.99340</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.93</td>\n",
       "      <td>12.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.038</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.99514</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.65</td>\n",
       "      <td>11.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.069</td>\n",
       "      <td>35.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.99632</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.075</td>\n",
       "      <td>15.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.99467</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.67</td>\n",
       "      <td>12.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>5.6</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.78</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.074</td>\n",
       "      <td>23.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99677</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.48</td>\n",
       "      <td>10.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.060</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.99474</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.64</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.081</td>\n",
       "      <td>16.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.99588</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.78</td>\n",
       "      <td>10.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.076</td>\n",
       "      <td>13.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.99622</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.118</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.99540</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.67</td>\n",
       "      <td>11.3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.053</td>\n",
       "      <td>24.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.99402</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11.3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.068</td>\n",
       "      <td>9.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.99470</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.053</td>\n",
       "      <td>24.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.99402</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11.3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.053</td>\n",
       "      <td>13.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.99362</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.074</td>\n",
       "      <td>32.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.99578</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.62</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.44</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.061</td>\n",
       "      <td>24.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99484</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.80</td>\n",
       "      <td>11.6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.44</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.066</td>\n",
       "      <td>22.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.99494</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.84</td>\n",
       "      <td>11.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.41</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.065</td>\n",
       "      <td>34.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99492</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.85</td>\n",
       "      <td>11.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>5.8</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.066</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.99483</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0.66</td>\n",
       "      <td>10.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.068</td>\n",
       "      <td>34.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99414</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.78</td>\n",
       "      <td>12.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.20</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.073</td>\n",
       "      <td>29.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.99770</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.077</td>\n",
       "      <td>26.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.99314</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.82</td>\n",
       "      <td>11.6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>5.4</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.089</td>\n",
       "      <td>16.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.99402</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.56</td>\n",
       "      <td>11.6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.068</td>\n",
       "      <td>28.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.99651</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.82</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "5               7.4             0.660         0.00             1.8      0.075   \n",
       "6               7.9             0.600         0.06             1.6      0.069   \n",
       "7               7.3             0.650         0.00             1.2      0.065   \n",
       "8               7.8             0.580         0.02             2.0      0.073   \n",
       "9               7.5             0.500         0.36             6.1      0.071   \n",
       "10              6.7             0.580         0.08             1.8      0.097   \n",
       "11              7.5             0.500         0.36             6.1      0.071   \n",
       "12              5.6             0.615         0.00             1.6      0.089   \n",
       "13              7.8             0.610         0.29             1.6      0.114   \n",
       "14              8.9             0.620         0.18             3.8      0.176   \n",
       "15              8.9             0.620         0.19             3.9      0.170   \n",
       "16              8.5             0.280         0.56             1.8      0.092   \n",
       "17              8.1             0.560         0.28             1.7      0.368   \n",
       "18              7.4             0.590         0.08             4.4      0.086   \n",
       "19              7.9             0.320         0.51             1.8      0.341   \n",
       "20              8.9             0.220         0.48             1.8      0.077   \n",
       "21              7.6             0.390         0.31             2.3      0.082   \n",
       "22              7.9             0.430         0.21             1.6      0.106   \n",
       "23              8.5             0.490         0.11             2.3      0.084   \n",
       "24              6.9             0.400         0.14             2.4      0.085   \n",
       "25              6.3             0.390         0.16             1.4      0.080   \n",
       "26              7.6             0.410         0.24             1.8      0.080   \n",
       "27              7.9             0.430         0.21             1.6      0.106   \n",
       "28              7.1             0.710         0.00             1.9      0.080   \n",
       "29              7.8             0.645         0.00             2.0      0.082   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1569            6.2             0.510         0.14             1.9      0.056   \n",
       "1570            6.4             0.360         0.53             2.2      0.230   \n",
       "1571            6.4             0.380         0.14             2.2      0.038   \n",
       "1572            7.3             0.690         0.32             2.2      0.069   \n",
       "1573            6.0             0.580         0.20             2.4      0.075   \n",
       "1574            5.6             0.310         0.78            13.9      0.074   \n",
       "1575            7.5             0.520         0.40             2.2      0.060   \n",
       "1576            8.0             0.300         0.63             1.6      0.081   \n",
       "1577            6.2             0.700         0.15             5.1      0.076   \n",
       "1578            6.8             0.670         0.15             1.8      0.118   \n",
       "1579            6.2             0.560         0.09             1.7      0.053   \n",
       "1580            7.4             0.350         0.33             2.4      0.068   \n",
       "1581            6.2             0.560         0.09             1.7      0.053   \n",
       "1582            6.1             0.715         0.10             2.6      0.053   \n",
       "1583            6.2             0.460         0.29             2.1      0.074   \n",
       "1584            6.7             0.320         0.44             2.4      0.061   \n",
       "1585            7.2             0.390         0.44             2.6      0.066   \n",
       "1586            7.5             0.310         0.41             2.4      0.065   \n",
       "1587            5.8             0.610         0.11             1.8      0.066   \n",
       "1588            7.2             0.660         0.33             2.5      0.068   \n",
       "1589            6.6             0.725         0.20             7.8      0.073   \n",
       "1590            6.3             0.550         0.15             1.8      0.077   \n",
       "1591            5.4             0.740         0.09             1.7      0.089   \n",
       "1592            6.3             0.510         0.13             2.3      0.076   \n",
       "1593            6.8             0.620         0.08             1.9      0.068   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "5                    13.0                  40.0  0.99780  3.51       0.56   \n",
       "6                    15.0                  59.0  0.99640  3.30       0.46   \n",
       "7                    15.0                  21.0  0.99460  3.39       0.47   \n",
       "8                     9.0                  18.0  0.99680  3.36       0.57   \n",
       "9                    17.0                 102.0  0.99780  3.35       0.80   \n",
       "10                   15.0                  65.0  0.99590  3.28       0.54   \n",
       "11                   17.0                 102.0  0.99780  3.35       0.80   \n",
       "12                   16.0                  59.0  0.99430  3.58       0.52   \n",
       "13                    9.0                  29.0  0.99740  3.26       1.56   \n",
       "14                   52.0                 145.0  0.99860  3.16       0.88   \n",
       "15                   51.0                 148.0  0.99860  3.17       0.93   \n",
       "16                   35.0                 103.0  0.99690  3.30       0.75   \n",
       "17                   16.0                  56.0  0.99680  3.11       1.28   \n",
       "18                    6.0                  29.0  0.99740  3.38       0.50   \n",
       "19                   17.0                  56.0  0.99690  3.04       1.08   \n",
       "20                   29.0                  60.0  0.99680  3.39       0.53   \n",
       "21                   23.0                  71.0  0.99820  3.52       0.65   \n",
       "22                   10.0                  37.0  0.99660  3.17       0.91   \n",
       "23                    9.0                  67.0  0.99680  3.17       0.53   \n",
       "24                   21.0                  40.0  0.99680  3.43       0.63   \n",
       "25                   11.0                  23.0  0.99550  3.34       0.56   \n",
       "26                    4.0                  11.0  0.99620  3.28       0.59   \n",
       "27                   10.0                  37.0  0.99660  3.17       0.91   \n",
       "28                   14.0                  35.0  0.99720  3.47       0.55   \n",
       "29                    8.0                  16.0  0.99640  3.38       0.59   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1569                 15.0                  34.0  0.99396  3.48       0.57   \n",
       "1570                 19.0                  35.0  0.99340  3.37       0.93   \n",
       "1571                 15.0                  25.0  0.99514  3.44       0.65   \n",
       "1572                 35.0                 104.0  0.99632  3.33       0.51   \n",
       "1573                 15.0                  50.0  0.99467  3.58       0.67   \n",
       "1574                 23.0                  92.0  0.99677  3.39       0.48   \n",
       "1575                 12.0                  20.0  0.99474  3.26       0.64   \n",
       "1576                 16.0                  29.0  0.99588  3.30       0.78   \n",
       "1577                 13.0                  27.0  0.99622  3.54       0.60   \n",
       "1578                 13.0                  20.0  0.99540  3.42       0.67   \n",
       "1579                 24.0                  32.0  0.99402  3.54       0.60   \n",
       "1580                  9.0                  26.0  0.99470  3.36       0.60   \n",
       "1581                 24.0                  32.0  0.99402  3.54       0.60   \n",
       "1582                 13.0                  27.0  0.99362  3.57       0.50   \n",
       "1583                 32.0                  98.0  0.99578  3.33       0.62   \n",
       "1584                 24.0                  34.0  0.99484  3.29       0.80   \n",
       "1585                 22.0                  48.0  0.99494  3.30       0.84   \n",
       "1586                 34.0                  60.0  0.99492  3.34       0.85   \n",
       "1587                 18.0                  28.0  0.99483  3.55       0.66   \n",
       "1588                 34.0                 102.0  0.99414  3.27       0.78   \n",
       "1589                 29.0                  79.0  0.99770  3.29       0.54   \n",
       "1590                 26.0                  35.0  0.99314  3.32       0.82   \n",
       "1591                 16.0                  26.0  0.99402  3.67       0.56   \n",
       "1592                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1593                 28.0                  38.0  0.99651  3.42       0.82   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "5         9.4        5  \n",
       "6         9.4        5  \n",
       "7        10.0        7  \n",
       "8         9.5        7  \n",
       "9        10.5        5  \n",
       "10        9.2        5  \n",
       "11       10.5        5  \n",
       "12        9.9        5  \n",
       "13        9.1        5  \n",
       "14        9.2        5  \n",
       "15        9.2        5  \n",
       "16       10.5        7  \n",
       "17        9.3        5  \n",
       "18        9.0        4  \n",
       "19        9.2        6  \n",
       "20        9.4        6  \n",
       "21        9.7        5  \n",
       "22        9.5        5  \n",
       "23        9.4        5  \n",
       "24        9.7        6  \n",
       "25        9.3        5  \n",
       "26        9.5        5  \n",
       "27        9.5        5  \n",
       "28        9.4        5  \n",
       "29        9.8        6  \n",
       "...       ...      ...  \n",
       "1569     11.5        6  \n",
       "1570     12.4        6  \n",
       "1571     11.1        6  \n",
       "1572      9.5        5  \n",
       "1573     12.5        6  \n",
       "1574     10.5        6  \n",
       "1575     11.8        6  \n",
       "1576     10.8        6  \n",
       "1577     11.9        6  \n",
       "1578     11.3        6  \n",
       "1579     11.3        5  \n",
       "1580     11.9        6  \n",
       "1581     11.3        5  \n",
       "1582     11.9        5  \n",
       "1583      9.8        5  \n",
       "1584     11.6        7  \n",
       "1585     11.5        6  \n",
       "1586     11.4        6  \n",
       "1587     10.9        6  \n",
       "1588     12.8        6  \n",
       "1589      9.2        5  \n",
       "1590     11.6        6  \n",
       "1591     11.6        6  \n",
       "1592     11.0        6  \n",
       "1593      9.5        6  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('winequality-red.csv') \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's inspect our data a bit.  In the cell below, perform some basic Exploratory Data Analysis on our dataset.  Get a feel for your data by exploring the descriptive statistics and creating at least 1 visualization to help you better understand this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4210584da0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8XGd97/HPT5slWdYu27IkS97jeI9lOw5NCCRkbwK3CTHQsLSQQtlL6WW5L8rlvtpbCrctSwuEAEkKgZCVAFnZsmLH8iavCd4lW7ZlyVps7Zrf/UMjR1FkayTNaEYn3/frNS+PZs6c8xtL+uqZ5zzPc8zdERGRYEmKdwEiIhJ9CncRkQBSuIuIBJDCXUQkgBTuIiIBpHAXEQkghbuISAAp3EVEAkjhLiISQCnxOnBhYaFXVFTE6/AiIhPSpk2bTrp70XDbxS3cKyoqqKqqitfhRUQmJDM7FMl26pYREQkghbuISAAp3EVEAkjhLiISQAp3EZEAUriLiASQwl1EJIAU7iIiAaRwFxEJoLjNUBUZrXs3HD7v8+9eM3OcKhFJXMO23M2szMx+b2a7zWynmX1yiG0uN7NmM9savn0pNuWKiEgkImm59wCfcffNZjYF2GRmT7v7rkHbPefuN0S/RBERGalhW+7uXufum8P3W4HdQEmsCxMRkdEb0QlVM6sAVgAbhnh6rZltM7PHzWzROV5/u5lVmVlVfX39iIsVEZHIRBzuZpYFPAh8yt1bBj29GSh392XAt4BHhtqHu9/h7pXuXllUNOxyxCIiMkoRhbuZpdIX7D9x94cGP+/uLe5+Onz/MSDVzAqjWqmIiEQsktEyBvwA2O3u/3aObaaHt8PMVof32xDNQkVEJHKRjJZ5E3AbsN3MtoYf+wIwE8DdvwvcDHzEzHqAdmCdu3sM6hURkQgMG+7u/jxgw2zzbeDb0SpKRETGRssPiIgEkMJdRCSAFO4iIgGkhcNE4kQLoEksqeUuIhJACncRkQBSuIuIBJDCXUQkgBTuIiIBpHAXEQkghbuISAAp3EVEAkjhLiISQAp3EZEAUriLiASQwl1EJIAU7iIiAaRwFxEJIIW7iEgAKdxFRAJI4S4iEkAKdxGRAFK4i4gEkMJdRCSAFO4iIgGkcBcRCSCFu4hIACncRUQCSOEuIhJACncRkQBSuIuIBNCw4W5mZWb2ezPbbWY7zeyTQ2xjZvZNM9trZtVmdlFsyhURkUikRLBND/AZd99sZlOATWb2tLvvGrDNtcC88G0N8J3wvyIiEgfDttzdvc7dN4fvtwK7gZJBm90E3ON91gO5ZlYc9WpFRCQiI+pzN7MKYAWwYdBTJUDNgK9ref0fABERGScRh7uZZQEPAp9y95bBTw/xEh9iH7ebWZWZVdXX14+sUhERiVhE4W5mqfQF+0/c/aEhNqkFygZ8XQocHbyRu9/h7pXuXllUVDSaekVEJAKRjJYx4AfAbnf/t3Ns9ijw3vComYuBZnevi2KdIiIyApGMlnkTcBuw3cy2hh/7AjATwN2/CzwGXAfsBdqAD0S/VBERidSw4e7uzzN0n/rAbRz4aLSKEhGRsdEMVRGRAFK4i4gEkMJdRCSAFO4iIgGkcBcRCSCFu4hIACncRUQCSOEuIhJACncRkQBSuIuIBJDCXUQkgBTuIiIBpHAXEQkghbuISAAp3EVEAkjhLiISQAp3EZEAUriLiASQwl1EJIAU7iIiAaRwFxEJIIW7iEgAKdxFRAJI4S4iEkAKdxGRAFK4i4gEkMJdRCSAFO4iIgGkcBcRCSCFu4hIACncRUQCSOEuIhJAw4a7mf3QzE6Y2Y5zPH+5mTWb2dbw7UvRL1NEREYiJYJt7gK+Ddxznm2ec/cbolKRiIiM2bAtd3d/Fmgch1pERCRKotXnvtbMtpnZ42a26FwbmdntZlZlZlX19fVROrSIiAwWjXDfDJS7+zLgW8Aj59rQ3e9w90p3rywqKorCoUVEZChjDnd3b3H30+H7jwGpZlY45spERGTUxhzuZjbdzCx8f3V4nw1j3a+IiIzesKNlzOynwOVAoZnVAv8IpAK4+3eBm4GPmFkP0A6sc3ePWcUiIjKsYcPd3d81zPPfpm+opIiIJAjNUBURCSCFu4hIACncRUQCSOEuIhJACncRkQBSuIuIBJDCXUQkgBTuIiIBpHAXEQkghbuISAAp3EVEAkjhLiISQAp3EZEAUriLiASQwl1EJIAU7iIiAaRwFxEJIIW7iEgAKdxFRAJI4S4iEkAKdxGRAFK4i4gEkMJdRCSAFO4iIgGkcBcRCSCFu4hIACncRUQCSOEuIhJACncRkQBSuIuIBJDCXUQkgIYNdzP7oZmdMLMd53jezOybZrbXzKrN7KLolykiIiMRScv9LuCa8zx/LTAvfLsd+M7YyxIRkbEYNtzd/Vmg8Tyb3ATc433WA7lmVhytAkVEZOSi0edeAtQM+Lo2/JiIiMRJNMLdhnjMh9zQ7HYzqzKzqvr6+igcWkREhhKNcK8FygZ8XQocHWpDd7/D3SvdvbKoqCgKhxYRkaFEI9wfBd4bHjVzMdDs7nVR2K+IiIxSynAbmNlPgcuBQjOrBf4RSAVw9+8CjwHXAXuBNuADsSpW3ti6ekJUHWwk5E6SDdUbKCL9hg13d3/XMM878NGoVSQyBHfncw9W89CWI1SW5/H2FSUKeJHz0AxVmRC+9+x+HtpyhJXleVQdOsUjW44Q8iHP24sICneZAH6/5wRffWIPNywt5oEPr+UtC6ZSdegUz7yiEVci56Jwl4T31Sf2MLcoi6/dvAwz48qFU5lblMWmQ6dwtd5FhqRwl4R2uKGNPcdauXVVGRlpyQCYGUtLc2g808XR5o44VzhyoZBz74bDnGideLXLxKFwl4T21K5jAFy9aPprHr+wOJskg+21zfEoa9S6e0N85v5tfOHh7fzguQOcauuKd0kSUAp3SWhP7jzGwuJsyvIzX/N45qQU5hRlseNo84Tpmuno7uUjP97Mw1uO8P5LKugOhbjrxYO0d/XGuzQJIIW7JKz61k6qDp3iqgunDfn84pKJ1TXz4/WH+M3u43zlpkV8+cZF/OWachrPdPGzjYfjXZoEkMJdEtZvdx/H/fVdMv0mWtfMw1uOsKw0h/eurQBgdlEWV14wlT+dOM3J053xLU4CR+EuCevJnccozctgYfGUIZ+fPIG6ZvaeaGXn0RZuWv7aBVOXleUCsOPIxPgDJROHwl0S0pnOHl7Y28DVi6Zj55mJumD6FBrPdNHc3j2O1Y3cI1uOkmRww7LXXuogNzONmfmZbFe4S5Qp3CUhVdc209Ub4s/mFp53u/L8yQAcbmwbj7JGxd35xbYjvGluIVOnpL/u+SUlOdQ1d1Dfqq4ZiR6FuySk7UeagFe7Lc5lek46qcnGoQQO982HT1HT2M7blw99DZvFJTkAar1LVCncJSFtq22mNC+D/Mlp590uOckozcvkcEPihvsjW46SnprE1YuHPjGck5FKeUHm2T9oItGgcJeEVF3bxNLSnIi2Lc/PpK65na6eUIyrGp3f7TnBm+cXkTXp3IuwLinJ4XhLJydaJsawTkl8CndJOI1nuqhpbGdp6fm7ZPrNLMgk5FDblHit99pTbRxpamft7ILzbndhcTYAe+tPj0dZ8gYw7HruIuOturaveyLSlvvMvL7Zq4cb2phdmBWzukZj48FGAFbPOn+452amkZuZysGGNi6ZMx6VRc+9G84/Cevda2aOUyUykFruknCqa5sx6+uqiETmpBQKsyYl5IiZlw40kp2ewoLpQ4/VH6iiYDKHGs4k/Jh9mRgU7pJwqmubmF04mSnpqRG/pjw/k8ONbQkXjC8daKSyIp/kpOGvGlVekElrRw+NZ7SYmIydwl0SiruzrbaZZRH2t/ebWZBJW1cvDacTJxhPnu5kX/0ZVs/Kj2j7ioK+MfuHEnjkj0wcCndJKMdbOqlv7Yy4v73fzPCqkYk03n3jgf7+9sjCvWjKJDJSkznYcCaWZckbhMJdEsq2/pOpw0xeGqxoyiTSkpM40tQei7JGZcOBRjJSk1k8I7I/VElmlBdkclAtd4kChbsklOraJlKS7OzQwEglmVGcm87RBAr3jQcbWTEzl7SUyH/NKgomc/J0J6c7e2JYmbwRKNwloVTXNjN/2hTSU5NH/NrS3Azqmtvp6Y3/ZKaWjm521bVE3CXTr7ygf1inumZkbBTukjDcneraZpaVjay/vd+M3Ay6e5199fEPxq2Hm3CHVRUjC/eS3AxSkkxdMzJmCndJGIca2mhu7454ZupgJbkZwKuToOJpa01T31j9EZ4YTklOojQvg0NqucsYKdwlYWwb4czUwQrDJ1UT4cIX22qamFOURfYIxur3K8vLpK65I2HXypGJQeEuCWN7bTOTUpKYP2342ZxD6T+pGu+lc92drTVNLB/hiJ9+pfmZ9IScPcdaolyZvJEo3CVhVNc2s2hGNqnJo/+xLMnNYFddS1xPqtaeaqfhTNeow70sr697aWtN/LuXZOJSuEtC6A05O442j7q/vV9JbgYd3aG4nlTtD+XRhntORipTJqUo3GVMFO6SEPaeOE1bV++oR8r06z+pGs+uma01TUxKSYposbChmBmleRkKdxkThbskhFdPpo6t5V44ZRKZaclxPam6raaJxSU5Y+peKsvPZH/9GZrbEvvC35K4FO6SEKprm5gyKYVZ4cWzRivJjEUzsuPWcu/uDbH9SPOou2T6lYbXqK/WpfdklCIKdzO7xsxeNrO9Zva5IZ5/v5nVm9nW8O2D0S9Vgqy6tpnFJTkkRbA07nAWl+Sw62h8Tqq+fKyVzp7QsBf2Hk5pXgZmfZOhREZj2HA3s2TgP4FrgQuBd5nZhUNsep+7Lw/f7oxynRJgnT297K5rYekY+9v7LSnJob27Ny4nVfv7yVeMMdzTU5OZU5SlfncZtUha7quBve6+3927gJ8BN8W2LHkjeflYK929PuI13M+l/wpO8eia2VbTRP7kNErDwxnHYnlZLltrmhLuAiQyMUQS7iVAzYCva8OPDfYXZlZtZg+YWdlQOzKz282sysyq6uvrR1GuBNG22r4QHu3M1MFmF2XF7aRq/+Qls7F3Ly0ry6XhTBe1pxJnpUuZOCIJ96F+Sgc3JX4JVLj7UuA3wN1D7cjd73D3SnevLCoqGlmlEljVNU0UTE47O4xxrJKT4nNStbWjm731p8d8MrVff9eOumZkNCIJ91pgYEu8FDg6cAN3b3D3zvCX3wdWRqc8eSOorm1maWlOVFq7/fpPqvaGxq9LY3ttM+6M+WRqvwXTpzApJUnhLqMSSbhvBOaZ2SwzSwPWAY8O3MDMigd8eSOwO3olSpC1dfXwpxOtYx7fPtirJ1VPR3W/57MlHMLLotS9lJqcxOKSHLYp3GUUhg13d+8BPgY8SV9o/9zdd5rZV8zsxvBmnzCznWa2DfgE8P5YFSzBsuNICyFnzDNTBzt7UrV2/LpmttU0MatwMrmZaVHb5/KyXLYfaaY7AS5AIhNLROPc3f0xd5/v7nPc/Z/Cj33J3R8N3/+8uy9y92Xu/hZ33xPLoiU4+tdeX1IS3ZZ7/0nV8ep3H+tKkOeyrCyXzp4QLx9rjep+Jfg0Q1Xiqrq2mRk56RRNmRTV/SaHr8M6XuF+rKWDE62dUeuS6aeTqjJaCneJq+rapqj3t/dbUjp+J1X7Z5Iun5kX1f2W5mVQMDlN4S4jpnCXuGlu6+ZgQ1vUZqYO1n9S9U8nYt+lsbW2ibTkJBYWj24lyHMxM5aFJzOJjITCXeKmfyXIaM1MHWxleV8retOhUzHZ/0BbDzexcEY2k1KSo77v5WW57Ks/TUuHVoiUyCncJW6qDjaSZNEbFz7YzPxMCrMmUXUwtuHevxLkWNeTOZflZbm4j+/IH5n4FO4SNy8dbGTRjByyJqXEZP9mxqqKPKoONcZk//12HGmmrauXVRX5Mdl//yebLYdj/wlEgkPhLnHR1RNiy+GmmAViv8qKfGoa2znW3BGzY6zf3/fHY83s2LyXnMxUFkybcvY4IpFQuEtcbD/STGdPiFUV0R1dMlj//mPZet9woIF5U7MozIrucM6B1s4poOpQI509vTE7hgSLwl3iYuPBvrCtjHHLfWFxNhmpyTHrd+/pDbHxQCMXzy6Iyf77rZ1TQEd3iG016neXyCjcJS42HmhkduHkqE9eGiw1OYkVM3Nj1nLfcbSFM129MQ/3i2cVYAZ/3NcQ0+NIcCjcZdyFQk7VoVMx72/vV1mRz66jLZzu7In6vtfv7wvb1bNi+15yMlO5sDibP+4/GdPjSHAo3GXcvXKileb2blbFOBD7rarII+SxGW2yfn8Dc6dmxfwTCMDa2QVsPtxER3fi9LvXt3bSqvH3CSk2Y9BEzmPjgb4uktXj1HJfMTOPJIOXDjRy6bzoXSSmv7/9HRcNdWGy6Fs7p4A7nz/A5sOnuGRO4bgccyjN7d38y+N7eH5vPTWN7Rh95zYunl3AnKLJUV2XX0ZPLXcZdxsONDItexJl+dG58tJwsialsLwslz+8HN1LO+4cp/72fqtm5ZNksD6O/e776k/zjv98gfuralhUnMMXr1vIpfOKONhwhh++cICndh3XNV8ThFruMq66e0M8+0o9Vy2aPq4tvCsWTuNrT77M8ZYOpmWnR2Wff3i5HjPGLdyz01NZUpLDi/sa+LtxOeJrVR1s5AN3bSQ1OYl7P3Tx2fMMkyelcOXCqfyyuo5nXqmnpzfEdUuK1YKPM7XcZVy9dKCRlo4e3nbhtHE97pUL+473290norbPx3fUUVmeF9Px7YNdOq+ILTVNNJ7pGrdjAhxtaudv/nsThVmTePRjb3rdCeSU5CTevnwGa+cU8MK+Bp7adXxc65PXU7jLuHp613EmpSRx6bzx7TOePy2L0rwMfrs7OqFz8OQZ9hxr5ZrFxcNvHEXXLJ5Ob8h5auexcTtmR3cvH/7xJjp7Qnz/vZWU5mUOuZ2ZccOSYlZX5PPMK/XsqWsZtxrl9RTuMm7cnad3HefSeYVkpo1vj6CZceXCaTy/9yTtXWMfbfJkOFyvXjS+n0AWzcimvCCTX2+vG7djfukXO6iubebfb13O3KlZ593WzLh+aTEzctK5f1MtTW3j+wlDXqVwl3Gzu66VI03t494l0++KhVPp7Anxwt6xjxV/YucxlpTknLMVGytmxrWLi3lxXwOnxqFr5vHtdfy8qpaPvWVuxN+31OQk1q2eSa87P9tYQ4+u/xoXOqH6BnHvhsPDbvPuNTNjWsPTu45jBm+9ID7hvmZWAVmTUvjtnuNcOYY/MHXN7Ww53MRnr14Qxeoid/2SYr77zD6e3n2cd1aWxew49a2dfOHh7SwpyeGTV84b0WsLsybxjuUl3FdVw3f+sI+PXzGy18vYqeUu4+bp3ce4aGbeuEz4GUpaShKXzS/kN7tPjKk1+dTOvn77axZPj1ZpI7K4JJvSvAwei2HXjLvz+YeqOdPVy7/fuozU5JFHxbKyXJaW5vCN3/6JHeN0LVt5lcJdxsWBk2fYcaQlbl0y/d6+vIT61s4xjeZ4dNtR5k7NYk7R+fufY8XMuH5JMS/sPUlzW2xmh96/qZbf7D7BP1y9gLlTR3/pwBuXzaAgK41P37c1oWbWvhEo3GVc3PXCAdKSk/gf4zSb81yuWDiNsvwM7nrh4Khev+nQKTYdOsW7V8e2C2s41y8tprvXeWhLbdT3XdPYxld+uYs1s/L5qzfNGtO+MtNS+NrNy/jTidN87cmXo1ShRELhLjHX3N7N/Ztq+fNlM5g6JToTiEYrOcl439oKXjrYOKqugjue3UdORiq3ropdX3cklpbmsqoijzufO0B3FE9YhkLO39+/DYCv37KMpKSxT0S6bH4Rt11czg+eP8CL+7Tw2XhRuEvM3bfxMG1dvfzVn1XEuxQAbqksIyM1mbtfPDii1+2rP81Tu47z3rXlTI7RpQFH4m8vn8uRpnZ+sfVo1PZ55/P72XCgkS/dcCFl+dEbCfT56y5gVuFkPnt/tS70PU4U7hJTPb0h7n7xEBfPzmfRjJx4lwNATkYqf7GyhF9sO0rD6c6IX3fnc/tJTU7ifZdUxK64Ebh8QREXTJ/Cd5/ZRyg09vVc1u9v4KtPvMxVF07jlsrSKFT4qsy0FP7tncs41tLBlx7ZofVnxoHCXWLq19vrONLUPua+22h7/yUV9PSG+OoTeyLa/nBDGw9uOsItK0vHdbmB8zEzPnL5HPaeOM3TY5x5e6y5g4/du5ny/Ey+/s5lMVkXZsXMPD55xTwe2XqUO587EPX9y2sp3CVmGs908X9+tZuFxdlcsTC+o2QGmzt1Ch+5fA4/r6o9O9v0XDp7evnovZtJT03ib98yd5wqjMz1S4qpKMjk/z62e9Trqrd19fDhH2+ivauX7922kuz01ChX+aqPvWUu1y2Zzj8/vjtqS0HI0OLfcShRd+pMF3/c38CG/Q0cbmyjrrmD+tZO0lOTyUxLpjgnnbL8TGYVTo7ZMgDuzhcf3k5zexf//derSY7Ciblo++QV83nmlXo+92A1K8pymXqO1SL/6de72X6kmTtuW0lJ7vgsUxyplOQkvnbLMtbdsZ7PP7Sdb71rxYha3Wc6e/jAjzZSXdvEf71nJfOmjX7YYySSkoz/d8tyahr/yCd+uoV7/noNK8tje5H0NyqFe0C0dfXwxI5j3F9Vy/oDDbhDZloyswonU5qXQVpKEp3dIU539vDC3gZ6/SRJBnOKslhSksOSkuj2h/9i61Ee33GM/3nNBSwszo7qvqMlLSWJ/7h1BTd86zk+eE8V31i3glmFk88+3xtyfvj8Ae754yE+dOksrloUn0lLw1lVkc9nrprPvz7xMmvnFPCeNeURva61o5u/umsjmw838Y11K8ZtUlZGWjLff28l6+74I++5cz3f+cuVvGXB1JgeMxFmaI83hfsE5t53LdL7q2r4dXUdZ7p6mZmfycffOo83zy9iaWnO2ZmFA3+4u3tDHG1qZ3ddKzuONvPQliP8qrqOXXUt3LqqjJXleWPqc31sex2fe6ialeV53H7Z7DG/z1iaOzWLb6xbwWfv38a133iWT14xn3lTs+gJOf/5+71sP9LMWy+Yyj9cc0G8Sz2vD182hw37G/nyozvp6XXeu7b8vN/DF/ee5LMPVHOspYNvrlvB9UvHd3XL6Tnp3P/hS3j/j17iQ3dX8eUbF/GeNTO1BnwURRTuZnYN8A0gGbjT3f9l0POTgHuAlUADcKu7H4xuqdKvprGNR7Yc4YHNtRxqaCMzLZnrlxRzS2UZqyqGD+bU5CTKCyZTXjCZqxdNo+ZUO1UHG3lsex33b6pldtFkbl5ZyvVLiikvmHzefQ0UCjnfeWYfX3vyZS6amcv3bluZkN0xg129aDrLSnP5nw9Wv+YE67TsSXzrXSu4YWniX3giKcn45roVfOq+Lfzjozt5fu9JPnftBcwufPWyd+7O5sOnuHdDDQ9urmVW4WR+/jdr49YtUjRlEj+9/WI++pPN/K9HdvCr6qP88zuWMDtOM3+DxoYbkmRmycArwNuAWmAj8C533zVgm78Flrr7h81sHfAOd7/1fPutrKz0qqqqsdYfE70hJ+R9t5+9VIM7mEFKkg35Sx7rj3Md3b1sOdzE+v0NPL3rOLvC62RfPDufW1aWcc3i6cOOu47kY+lNy2fw6+o67quqYdOhvotJL5qRzZ/NLeTi2QUsLsmhMCvtNf8H7k59aye/3l7H3S8e5GBDGzcum8G/3ryU9NTkMbzr0b+X0X4/3J39J89wprOHrp4QC4uzYzqePRbvw935wfMH+OoTe+judcryM1gwLZvWjm6ONLVTe6qdjNRk3r1mJn9/1QIy0sb+PRrr+3B37ttYwz89tpu2rl6uXDiV96wp55I5BaSMYk2bgbp7Q9Q1dXD3Hw/S1NbFqbbus/+2d/XS1tVDV28I974JbhmpyWSkJVMwOY3pOelMz05nWk46xTnpzMjJoCQvg+nZ6WOuayzMbJO7Vw63XSQ/uauBve6+P7zjnwE3AbsGbHMT8OXw/QeAb5uZeYwHs7o7IX81jHtDTq87HV29nAl/49q6emnr6qW9q4cznb20dnTT3N5Dc3v32VvLgPvN7d20n2MNjCTr66edlJJMWkoS6SlJZKQls35/A9kZKeRkpJ69ZaeH/81IZVJKEmkpSaQm9/2bkmRna+0NOT29TmdPKFxbNydaOqlr7uDAydO8cvw0e0+cpqs3hBmsnJnHF69byDWLp0d1kgn0XS7tnavKeOeqMmpPtfH49mM8ves4P3zhAN97dj/Qdz3SGbnppCQlYdb3KaKloweAi2bm8pmrFkyIlu5QzCxu68VEi5nxwUtnc92SYn675wTPvHyC2lNtZGeksnhGDp+4Yh7XLSkmKwEmYfUzM9atnslbL5jKD144wP1VtTy58ziZacksL8tlcUkOM3LSmZ6TzuRJKWSkJtMbcjp6QnR299LRE6Kjq5f6052caOngeEsnx1s7ONbcwfGWDgZOATBgSnoKuZlp5E9OoyQvg9TkJJKt7yLf7d19eXHydCf768/w4r4GWsM/3/2Sk4zp2emU5GVQmtsX+IVZk8jN7Pt9zx34e5+cREpyEqnJRmpyXwaM16fZSL7DJUDNgK9rgTXn2sbde8ysGSgAoj7X+PHtdXziZ1vCgT76/UxOSz4bvjkZqZQXZJ4N5qz0lLOt9OqaJsyMkPcFcFdPiM6eEJ09vXT2hDjT2cu22iZa2rtp6eihNwqTSfoV56Qzf9oULp1fyOqKfCrL88nJjN0wtYFK8zL50GWz+dBls2nv6mXL4VO8cryVgw1tHGvuoCf8B3XFzFzmFmVRWZHP4iiflJXRm5GbwW0Xl3PbxZGdXE0EU7PT+fy1C/m7t83nd7tPsOFAIxsPNnLXiwfp6olsiYXs9BSmZaczLTudtXMKKM3NoDQvk5ePt5KbkUpOZiopSUO3us/1CeNMZw91zR0cDX/yOdLUxpFT7Rxpamf9/gaODfoDMhwz+Mib58T8PE4k3TK3AFe7+wfDX98GrHb3jw/YZmd4m9rw1/vC2zQM2tftwO3hLxcAE2EloUI6oEeMAAAFbElEQVRi8EcqDoLyPiA470XvI7FMlPdR7u5Fw20UScu9Fhi4SlIpMHgxi/5tas0sBcgBGgfvyN3vAO6I4JgJw8yqIunfSnRBeR8QnPei95FYgvI++kVyVmAjMM/MZplZGrAOeHTQNo8C7wvfvxn4Xaz720VE5NyGbbmH+9A/BjxJ31DIH7r7TjP7ClDl7o8CPwD+28z20tdiXxfLokVE5PwiOmXu7o8Bjw167EsD7ncAt0S3tIQxobqRziMo7wOC8170PhJLUN4HEMEJVRERmXi0KqSISAAp3M/BzNLN7CUz22ZmO83sf8e7prEws2Qz22Jmv4p3LaNlZgfNbLuZbTWzxJzeHAEzyzWzB8xsj5ntNrO18a5ppMxsQfj70H9rMbNPxbuu0TCzT4d/x3eY2U/NLL7XgowSdcucg/VNsZzs7qfNLBV4Hviku6+Pc2mjYmZ/B1QC2e5+Q7zrGQ0zOwhUuvtEGIt8TmZ2N/Ccu98ZHoGW6e5N8a5rtMJLlBwB1rj7oXjXMxJmVkLf7/aF7t5uZj8HHnP3u+Jb2dip5X4O3ud0+MvU8G1C/iU0s1LgeuDOeNfyRmdm2cBl9I0ww927JnKwh10B7JtowT5ACpARnqOTyevn8UxICvfzCHdlbAVOAE+7+4Z41zRK/wH8AxDZHO7E5cBTZrYpPNt5IpoN1AM/CneT3WlmkS+9mZjWAT+NdxGj4e5HgK8Dh4E6oNndn4pvVdGhcD8Pd+919+X0zcpdbWaL413TSJnZDcAJd98U71qi4E3ufhFwLfBRM7ss3gWNQgpwEfAdd18BnAE+F9+SRi/crXQjcH+8axkNM8ujb+HDWcAMYLKZ/WV8q4oOhXsEwh+b/wBcE+dSRuNNwI3h/uqfAW81sx/Ht6TRcfej4X9PAA/Tt2LpRFML1A74FPgAfWE/UV0LbHb3iXpB1CuBA+5e7+7dwEPAJXGuKSoU7udgZkVmlhu+n0HfD8Ge878q8bj759291N0r6Pv4/Dt3n3AtEzObbGZT+u8DVwE74lvVyLn7MaDGzBaEH7qC1y6fPdG8iwnaJRN2GLjYzDLDgyiuAHbHuaaoSJxFnRNPMXB3eCRAEvBzd5+wwwgDYBrwcHid+BTgXnd/Ir4ljdrHgZ+EuzT2Ax+Icz2jYmaZ9F3E52/iXctoufsGM3sA2Az0AFsIyExVDYUUEQkgdcuIiASQwl1EJIAU7iIiAaRwFxEJIIW7iEgAKdxFzsHMKsxsR/h+pZl9M3z/cjMLxEQXCS6NcxeJgLtXAf3LDF8OnAZejFtBIsNQy10Cycy+aGYvm9lvwmt0/72Z/cHMKsPPF4aXZOhvoT9nZpvDt9e1ysOt9V+ZWQXwYeDT4XXMLzWzA+FloTGz7PC686nj9mZFhqCWuwSOma2kb6mFFfT9jG8Gzrdw2gngbe7eYWbz6JtOXznUhu5+0My+C5x296+Hj/cH+pZUfiR83AfD65SIxI1a7hJElwIPu3ubu7cAjw6zfSrwfTPbTt/qhheO8Hh38uoSAh8AfjTC14tEnVruElRDravRw6sNmoGXUvs0cBxYFn6+Y0QHcn8h3LXzZiDZ3SfcgmYSPGq5SxA9C7zDzDLCK0n+efjxg8DK8P2bB2yfA9S5ewi4DUgeZv+twJRBj91DX3eOWu2SEBTuEjjuvhm4D9gKPAg8F37q68BHzOxFoHDAS/4LeJ+ZrQfm03cBjfP5JX1/PLaa2aXhx34C5DGxl7+VANGqkBJ4ZvZlBpwAjdExbgZucvfbYnUMkZFQn7vIGJnZt+i7ItF18a5FpJ9a7iIiAaQ+dxGRAFK4i4gEkMJdRCSAFO4iIgGkcBcRCSCFu4hIAP1/7JV1rP2D0RAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df['quality']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.319637</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>2.538806</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>15.874922</td>\n",
       "      <td>46.467792</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>3.311113</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>5.636023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.741096</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>10.460157</td>\n",
       "      <td>32.895324</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.154386</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>1.065668</td>\n",
       "      <td>0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
       "mean        8.319637          0.527821     0.270976        2.538806   \n",
       "std         1.741096          0.179060     0.194801        1.409928   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.390000     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.260000        2.200000   \n",
       "75%         9.200000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
       "mean      0.087467            15.874922             46.467792     0.996747   \n",
       "std       0.047065            10.460157             32.895324     0.001887   \n",
       "min       0.012000             1.000000              6.000000     0.990070   \n",
       "25%       0.070000             7.000000             22.000000     0.995600   \n",
       "50%       0.079000            14.000000             38.000000     0.996750   \n",
       "75%       0.090000            21.000000             62.000000     0.997835   \n",
       "max       0.611000            72.000000            289.000000     1.003690   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
       "mean      3.311113     0.658149    10.422983     5.636023  \n",
       "std       0.154386     0.169507     1.065668     0.807569  \n",
       "min       2.740000     0.330000     8.400000     3.000000  \n",
       "25%       3.210000     0.550000     9.500000     5.000000  \n",
       "50%       3.310000     0.620000    10.200000     6.000000  \n",
       "75%       3.400000     0.730000    11.100000     6.000000  \n",
       "max       4.010000     2.000000    14.900000     8.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Question:_** Based on your findings during your Eploratory Data Analysis, do you think that we need to do any sort of preprocessing on this dataset? Why or why not?\n",
    "\n",
    "Write your answer below this line:\n",
    "________________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "### Preprocessing our Data\n",
    "\n",
    "Now, we'll perform any necessary preprocessing on our dataset before training our model.  We'll start by isolating the target variable that we are trying to predict.  In the cell below:\n",
    "\n",
    "* Store the data in the `quality` column inside the `labels` variable\n",
    "* Drop the `quality` column from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       5\n",
       "1       5\n",
       "2       5\n",
       "3       6\n",
       "4       5\n",
       "5       5\n",
       "6       5\n",
       "7       7\n",
       "8       7\n",
       "9       5\n",
       "10      5\n",
       "11      5\n",
       "12      5\n",
       "13      5\n",
       "14      5\n",
       "15      5\n",
       "16      7\n",
       "17      5\n",
       "18      4\n",
       "19      6\n",
       "20      6\n",
       "21      5\n",
       "22      5\n",
       "23      5\n",
       "24      6\n",
       "25      5\n",
       "26      5\n",
       "27      5\n",
       "28      5\n",
       "29      6\n",
       "       ..\n",
       "1569    6\n",
       "1570    6\n",
       "1571    6\n",
       "1572    5\n",
       "1573    6\n",
       "1574    6\n",
       "1575    6\n",
       "1576    6\n",
       "1577    6\n",
       "1578    6\n",
       "1579    5\n",
       "1580    6\n",
       "1581    5\n",
       "1582    5\n",
       "1583    5\n",
       "1584    7\n",
       "1585    6\n",
       "1586    6\n",
       "1587    6\n",
       "1588    6\n",
       "1589    5\n",
       "1590    6\n",
       "1591    6\n",
       "1592    6\n",
       "1593    6\n",
       "1594    5\n",
       "1595    6\n",
       "1596    6\n",
       "1597    5\n",
       "1598    6\n",
       "Name: quality, Length: 1599, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['quality'] \n",
    "df = df.drop('quality', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've isolated our labels, we'll need to normalize our dataset (also referred to as _scaling_).  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create a `StandardScaler()` object.\n",
    "* Transform the data in `labels_removed_df` using the scaler object's `fit_transform()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(X)\n",
    "scaled_X = scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Testing, and Cross Validation\n",
    "\n",
    "Normally, we would also split our data into training and testing sets.  However, since we'll be making use of **_Cross Validation_** when using `GridSearchCV`, we'll also want to make use of it with our baseline model to ensure that things are equal.  Recall that we do not need to split our data into training and testing sets when using cross validation, since the cross validation will take care of that for us.  \n",
    "\n",
    "### Creating a Baseline Model: Decision Trees\n",
    "\n",
    "In the cell below:\n",
    "* Create a `DecisionTreeClassifier` object.  \n",
    "* Get the `cross_val_score` for this model, with the `cv` parameter set to `3`.\n",
    "* Calculate and print the mean cross-validation score from our model.\n",
    "\n",
    "**_Note:_** If you need a refresher on how to use `cross_val_score`, check out the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation Score: 44.22%\n"
     ]
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_cv_score = cross_val_score(dt_clf, X, y, cv=3)\n",
    "mean_dt_cv_score = dt_cv_score.mean()\n",
    "\n",
    "print(\"Mean Cross Validation Score: {:.4}%\".format(mean_dt_cv_score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search: Decision Trees\n",
    "\n",
    "Take a second to interpret the results of our cross-validation score.  How well did our model do? How does this compare to a naive baseline level of accuracy (random guessing)?\n",
    "\n",
    "Write your answer below this line:\n",
    "________________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "\n",
    "### Creating A Parameter Grid\n",
    "\n",
    "So far, our model has not have stellar performance. However, we've yet to modify the hyperparameters of the model.  Each dataset is different, and the chances that the best possible parameters for a given dataset also happen to be the default parameters set by by sklearn at instantiation is very low.  \n",
    "\n",
    "This means that we need to try **_Hyperparameter Tuning_**.  There are several strategies for searching for optimal hyperparameters--the one we'll be using, **_Combinatoric Grid Searching_**, is probably the most popular, because it performs an exhaustive search of all possible combinations.  \n",
    "\n",
    "The sklearn module we'll be using to accomplish this is `GridSearchCV`, which can be found inside of `sklearn.model_selection`.\n",
    "\n",
    "Take a minute to look at sklearn's user guide for [GridSearchCV](http://scikit-learn.org/stable/modules/grid_search.html#grid-search), and then complete the following task.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Complete the `param_grid` dictionary.  In this dictionary, each key represents a parameter we want to tune, whereas the corresponding value is an array of every parameter value we'd like to check for that parameter.  For instance, if we would like try out the values `2`, `5`, and `10` for `min_samples_split`, our `param_grid` dictionary would include `\"min_samples_split\": [2, 5, 10]`.\n",
    "* Normally, you would have to just try different values to search through for each parameter.  However, in order to limit the complexity of this lab, the parameters and values to search through have been provided for you.  You just need to turn them into key-value pairs inside of the `param_grid` dictionary. Complete `param_grid` so that it tests the following values for each corresponding parameter:\n",
    "    * For `\"criterion\"`, try values of `\"gini\"` and `\"entropy\"`.\n",
    "    * For `\"max_depth\"`, try `None`, as well as `2, 3, 4, 5` and `6`.\n",
    "    * For `min_samples_split`, try `2, 5`, and `10`.\n",
    "    * For `\"min_samples_leaf\"`, try `1, 2, 3, 4, 5` and `6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_param_grid = {'criterion':['gini','entropy'], 'max_depth': [None, 2,3,4,5,6], 'min_samples_split':[2,5,10], 'min_samples_leaf':[1,2,3,4,5,6]\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our parameter grid set up, we can create and use our `GridSearchCV` object.  Before we do, let's briefly think about the particulars of this model. \n",
    "\n",
    "Grid Searching works by training a model on the data for each unique combination of parameters, and then returning the parameters of the model that performed best. In order to protect us from randomness, it is common to implement K-Fold Cross Validation during this step.  For this lab, we'll set K = 3, meaning that we'll actually train 3 different models for each unique combination of parameters.  \n",
    "\n",
    "Given our `param_grid` and the knowledge that we're going to use Cross Validation with a value of 3, how many different Decision Trees will our `GridSearchCV` object have to train in order to try every possible combination and find the best parameter choices?\n",
    "\n",
    "Calculate and print your answer in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search will have to search through 648 different permutations.\n"
     ]
    }
   ],
   "source": [
    "num_decision_trees = 2 * 6 * 3 * 6 * 3\n",
    "print(\"Grid Search will have to search through {} different permutations.\".format(num_decision_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's alot of Decision Trees! Decision Trees are generally pretty quick to train, but that isn't the case with every type of model we could want to tune.  Be aware that if you set a particularly large search space of parameters inside your parameter grid, then Grid Searching could potentially take a very long time. \n",
    "\n",
    "Let's create our `GridSearchCV` object and fit it.  In the cell below:\n",
    "* Create a `GridSearchCV` object.  Pass in our model, the parameter grid, and `cv=3` to tell the object to use 3-Fold Cross Validation. Also pass in `return`\n",
    "* Call our grid search object's `fit()` method and pass in our data and labels, just as if we were using regular cross validation.  \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'criterion': ['gini', 'entropy'], 'max_depth': [None, 2, 3, 4, 5, 6], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 3, 4, 5, 6]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_grid_search = GridSearchCV(dt_clf, dt_param_grid, cv=3, return_train_score=True)\n",
    "dt_grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Best Parameters\n",
    "\n",
    "Now that we have fit our model using Grid Search, we need to inspect it to discover the optimal combination of parameters.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Calculate the the mean training score.  An array of training score results can be found inside of the `.cv_results_` dictionary, with the key `mean_train_score`.\n",
    "* Calcuate the testing score using the our grid search model's `.score()` method by passing in our data and labels. \n",
    "* Examine the appropriate attribute to discover the best estimator parameters found during the grid search. \n",
    "\n",
    "**_HINT:_** If you're unsure what attribute this is stored in, take a look at sklearn's [GridSearchCV Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Score: 67.15%\n",
      "Mean Testing Score: 66.04%\n",
      "Best Parameter Combination Found During Grid Search:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 5,\n",
       " 'min_samples_leaf': 6,\n",
       " 'min_samples_split': 10}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_gs_training_score = dt_grid_search.cv_results_['mean_train_score'].mean()\n",
    "dt_gs_testing_score = dt_grid_search.score(X,y)\n",
    "dt_gs_testing_score\n",
    "\n",
    "print(\"Mean Training Score: {:.4}%\".format(dt_gs_training_score * 100))\n",
    "print(\"Mean Testing Score: {:.4}%\".format(dt_gs_testing_score * 100))\n",
    "print(\"Best Parameter Combination Found During Grid Search:\")\n",
    "dt_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Question:_** What effect, if any, did our parameter tuning have on model performance? Will GridSearchCV always discover a perfectly (global) optimal set of parameters? Why or why not?\n",
    "________________________________________________________________________________________________________________________________\n",
    "  \n",
    "\n",
    "### Tuning More Advanced Models: Random Forests\n",
    "\n",
    "Now that we have some experience with Grid Searching through parameter values for a Decision Tree Classifier, let's try our luck with a more advanced model and tune a _Random Forest Classifier_.  \n",
    "\n",
    "We'll start by repeating the same process we did for our Decision Tree Classifier, except with a Random Forest Classifier instead. \n",
    "\n",
    "In the cell below:\n",
    "* Create a `RandomForestClassifier` object.\n",
    "* Use Cross Validation with `cv=3` to generate a baseline score for this model type, so that we have something to compare our tuned model performance to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation Score for Random Forest Classifier: 51.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier() \n",
    "mean_rf_cv_score = cross_val_score(rf_clf, X, y, cv=3)\n",
    "mean_rf_cv_score = mean_rf_cv_score.mean()\n",
    "print(\"Mean Cross Validation Score for Random Forest Classifier: {:.4}%\".format(mean_rf_cv_score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our baseline score, we'll create a parameter grid specific to our Random Forest Classifier.  \n",
    "\n",
    "Again--in a real world situation, you will need to decide what parameters to tune, and be very thoughtful about what values to test for each parameter.  However, since this is a lab, we have provided the following table in the interest of simplicity.  Complete the `rf_param_grid` dictionary with the following key value pairs:\n",
    " \n",
    " \n",
    " |     Parameter     |         Values         |\n",
    "|:-----------------:|:----------------------:|\n",
    "|    n_estimators   |      [10, 30, 100]     |\n",
    "|     criterion     |   ['gini', 'entropy']  |\n",
    "|     max_depth     | [None, 2, 6, 10] |\n",
    "| min_samples_split |       [5, 10]       |\n",
    "|  min_samples_leaf |   [3, 6]   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = { 'n_estimators': [10, 30, 100],\n",
    "'criterion' : ['gini', 'entropy'],\n",
    "'max_depth' : [None, 2, 6, 10],\n",
    "'min_samples_split' : [5, 10],\n",
    "'min_samples_leaf' : [3, 6] } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have our parameter grid, we can grid search through it with our Random Forest. \n",
    "\n",
    "In the cell below, follow the process we used with Decision Trees above to grid search for the best parameters for our Random Forest Classifier.  \n",
    "\n",
    "When creating your `GridSearchCV` object,  pass in:\n",
    "* our Random Forest Classifier\n",
    "* The parameter grid for our Random Forest Classifier\n",
    "* `cv=3` \n",
    "* **_Do not_** pass in `return_train_score` as we did with our Decision Trees example above.  In the interest of runtime, we'll only worry about testing accuracy this time. \n",
    "\n",
    "\n",
    "**_NOTE:_** The runtime on the following cell will be over a minute on most computers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 58.85%\n",
      "Total Runtime for Grid Search on Random Forest Classifier: 33.18 seconds\n",
      "\n",
      "Optimal Parameters: {'criterion': 'entropy', 'max_depth': 6, 'min_samples_leaf': 6, 'min_samples_split': 5, 'n_estimators': 30}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "rf_grid_search = GridSearchCV(rf_clf, rf_param_grid, cv=3)\n",
    "rf_grid_search.fit(X, y)\n",
    "\n",
    "print(\"Testing Accuracy: {:.4}%\".format(rf_grid_search.best_score_ * 100))\n",
    "print(\"Total Runtime for Grid Search on Random Forest Classifier: {:.4} seconds\".format(time.time() - start))\n",
    "print(\"\")\n",
    "print(\"Optimal Parameters: {}\".format(rf_grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Our Results\n",
    "\n",
    "Did tuning the hyperparameters of our Random Forest Classifier improve model performance? Is this performance increase significant? Which model did better? If you had to choose, which model would you put into production? Explain your answer. \n",
    "\n",
    "Write your answer below this line:\n",
    "________________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "### Tuning Gradient Boosted Trees (AdaBoost)\n",
    "\n",
    "The last model we'll tune in this lab is an AdaBoost Classifier, although tuning this model will generally be similar to tuning other forms of Gradient Boosted Tree (GBT) models.  \n",
    "\n",
    "In the cell below, create an AdaBoost Classifier Object.  Then, as we did with the previous two examples, fit the model using using Cross Validation to get a baseline testing accuracy so we can see how an untuned AdaBoost model performs on this task.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_clf = None\n",
    "adaboost_mean_cv_score = None\n",
    "\n",
    "# print(\"Mean Cross Validation Score for AdaBoost: {:.4}%\".format(adaboost_mean_cv_score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, onto creating the parameter grid for AdaBoost.  \n",
    "\n",
    "Complete the `adaboost_param_grid` dictionary by adding in the following key-value pairs:\n",
    "\n",
    "|   Parameters  |      Values     |\n",
    "|:-------------:|:---------------:|\n",
    "|  n_estimators |  [50, 100, 250] |\n",
    "| learning_rate | [1.0, 0.5, 0.1] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_param_grid = {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great.  Now, for the finale--use Grid Search to find optimal parameters for AdaBoost, and see how the model performs overall!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_grid_search = None\n",
    "# adaboost_grid_search.fit(None, None)\n",
    "\n",
    "# print(\"Testing Accuracy: {:.4}%\".format(adaboost_grid_search.best_score_ * 100))\n",
    "# print(\"Total Runtime for Grid Search on AdaBoost: {:.4} seconds\".format(time.time() - start))\n",
    "# print(\"\")\n",
    "# print(\"Optimal Parameters: {}\".format(adaboost_grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we learned:\n",
    "\n",
    "* How to iteratively search for optimal model parameters using `GridSearhCV`\n",
    "* How to tune model parameters for Decision Trees, Random Forests, and AdaBoost models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
